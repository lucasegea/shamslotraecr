"""
Script para scrapear los productos sin categor√≠a de OfertasB
Este script:
1. Crea una categor√≠a "Sin categor√≠a" en Supabase
2. Recorre todas las p√°ginas de productos
3. Identifica productos "sin categor√≠a"
4. Cuenta el total de productos y actualiza la base de datos
"""
import os
import time
import re
import sys
import random
import argparse
import httpx
import tqdm
import io
import hashlib
from typing import Dict, List, Any, Optional
from bs4 import BeautifulSoup
from datetime import datetime
from supabase import create_client
from dotenv import load_dotenv
import uuid
import base64
from utils.price import parse_price

# Cargar variables de entorno
load_dotenv()

# Constantes
BASE_URL = "https://www.ofertasb.com"
SLEEP_MIN = 1.0  # Tiempo m√≠nimo de espera entre solicitudes (segundos)
SLEEP_MAX = 2.0  # Tiempo m√°ximo de espera entre solicitudes (segundos)
USER_AGENT = "Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/96.0.4664.110 Safari/537.36"

# Carpeta de destino para im√°genes en Supabase Storage
IMAGE_BUCKET = "product-images"  # Nombre del bucket en Supabase Storage
UNCATEGORIZED_FOLDER = "sin categoria"  # Carpeta espec√≠fica para productos sin categor√≠a

class UncategorizedScraper:
    def __init__(self):
        """Inicializar el scraper y la conexi√≥n a Supabase"""
        print("Inicializando scraper...")
        
        # Inicializar la sesi√≥n HTTP con timeout y reintentos
        self.session = httpx.Client(
            timeout=30.0,
            headers={"User-Agent": USER_AGENT},
            follow_redirects=True
        )
        
        # Inicializar conexi√≥n a Supabase
        self.setup_supabase()
        
        # Mapeo para IDs internos de categor√≠as
        self.category_map = {}
        
        # Cache de productos existentes para evitar duplicados
        self.existing_products = {}
        
        # Estad√≠sticas
        self.stats = {
            "total_products": 0,
            "uncategorized_products": 0,
            "new_products": 0,
            "existing_products": 0,
            "errors": 0,
            "images_saved": 0,
            "image_errors": 0
        }
    
    def setup_supabase(self):
        """Configurar la conexi√≥n a Supabase"""
        try:
            url = os.getenv("SUPABASE_URL")
            key = os.getenv("SUPABASE_SERVICE_ROLE_KEY")
            
            if not url or not key:
                raise ValueError("Las variables de entorno SUPABASE_URL y SUPABASE_SERVICE_ROLE_KEY son requeridas")
            
            self.supabase = create_client(url, key)
            print("‚úÖ Conexi√≥n a Supabase establecida")
            
        except Exception as e:
            print(f"‚ùå Error al conectar con Supabase: {str(e)}")
            sys.exit(1)

    def create_uncategorized_category(self):
        """Crear la categor√≠a 'Sin categor√≠a' si no existe"""
        try:
            # Verificar si ya existe la categor√≠a
            result = self.supabase.table("categories").select("*").eq("name", "Sin categor√≠a").execute()
            
            if result.data:
                print(f"‚úÖ La categor√≠a 'Sin categor√≠a' ya existe con ID: {result.data[0]['id']}")
                uncategorized_category = result.data[0]
            else:
                # Crear la categor√≠a
                category_data = {
                    "external_id": "uncategorized",
                    "name": "Sin categor√≠a",
                    "source_url": f"{BASE_URL}/productos_cat.asp",
                    "last_crawled_at": datetime.utcnow().isoformat(),
                    "seller_id": 1
                }
                
                result = self.supabase.table("categories").insert(category_data).execute()
                
                if not result.data:
                    raise ValueError("Error al crear la categor√≠a 'Sin categor√≠a'")
                
                uncategorized_category = result.data[0]
                print(f"‚úÖ Categor√≠a 'Sin categor√≠a' creada con ID: {uncategorized_category['id']}")
            
            # Guardar la referencia a la categor√≠a
            self.uncategorized_category = uncategorized_category
            return uncategorized_category
            
        except Exception as e:
            print(f"‚ùå Error creando la categor√≠a 'Sin categor√≠a': {str(e)}")
            sys.exit(1)
    
    def load_categories_map(self):
        """Cargar mapa de IDs externos a IDs internos de categor√≠as"""
        try:
            result = self.supabase.table("categories").select("id, external_id, name").execute()
            
            if not result.data:
                print("‚ö†Ô∏è No se encontraron categor√≠as en la base de datos")
                return {}
            
            # Crear el mapa de IDs
            self.category_map = {
                category["external_id"]: {
                    "id": category["id"],
                    "name": category["name"]
                }
                for category in result.data
            }
            
            print(f"‚úÖ Se cargaron {len(self.category_map)} categor√≠as")
            return self.category_map
            
        except Exception as e:
            print(f"‚ùå Error cargando categor√≠as: {str(e)}")
            return {}
    
    def load_existing_products(self):
        """Cargar productos existentes para evitar duplicados"""
        try:
            print("Cargando productos existentes...")
            
            # Implementamos paginaci√≥n para obtener todos los productos
            page_size = 1000
            current_page = 0
            all_products = []
            total_fetched = 0
            
            while True:
                # Calcular offset para esta p√°gina
                offset = current_page * page_size
                
                # Consultar esta p√°gina de productos
                result = self.supabase.table("products") \
                    .select("external_product_id") \
                    .range(offset, offset + page_size - 1) \
                    .execute()
                
                if not result.data or len(result.data) == 0:
                    # No hay m√°s productos, salir del bucle
                    break
                
                # Agregar productos a la lista
                all_products.extend(result.data)
                total_fetched += len(result.data)
                print(f"  Cargados {total_fetched} productos hasta ahora...")
                
                # Comprobar si hemos llegado al final
                if len(result.data) < page_size:
                    break
                
                # Avanzar a la siguiente p√°gina
                current_page += 1
            
            # Convertir a diccionario para b√∫squeda r√°pida
            self.existing_products = {
                str(product["external_product_id"]): True 
                for product in all_products
            }
            
            print(f"‚úÖ Total de productos existentes cargados: {len(self.existing_products)}")
            return self.existing_products
            
        except Exception as e:
            print(f"‚ùå Error cargando productos existentes: {str(e)}")
            return {}
    
    def fetch_page(self, url):
        """Obtener el contenido HTML de una p√°gina"""
        try:
            print(f"Descargando: {url}")
            
            # A√±adir un retraso aleatorio para evitar detecci√≥n
            time.sleep(random.uniform(SLEEP_MIN, SLEEP_MAX))
            
            # Hacer la solicitud HTTP
            response = self.session.get(url)
            
            if response.status_code != 200:
                print(f"‚ö†Ô∏è C√≥digo de estado HTTP inesperado: {response.status_code}")
                return None
            
            # Comprobar si el HTML es v√°lido
            html_content = response.text
            if not html_content or len(html_content) < 1000:
                print(f"‚ö†Ô∏è Contenido HTML demasiado peque√±o: {len(html_content)} bytes")
                return None
            
            return html_content
            
        except Exception as e:
            print(f"‚ùå Error obteniendo la p√°gina {url}: {str(e)}")
            return None
    
    def get_total_pages(self):
        """Determinar el n√∫mero total de p√°ginas a procesar"""
        try:
            print("Determinando el n√∫mero total de p√°ginas...")
            first_page_url = f"{BASE_URL}/productos_cat.asp"
            
            html = self.fetch_page(first_page_url)
            if not html:
                return 0
                
            soup = BeautifulSoup(html, 'html.parser')
            
            # Buscar enlaces de paginaci√≥n
            pagination_links = soup.find_all("a", href=lambda href: href and "pagina=" in href)
            
            if not pagination_links:
                print("‚ö†Ô∏è No se encontraron enlaces de paginaci√≥n")
                return 1  # Asumimos que solo hay una p√°gina
            
            # Encontrar el n√∫mero de p√°gina m√°s alto
            max_page = 1
            
            for link in pagination_links:
                href = link.get("href", "")
                match = re.search(r"pagina=(\d+)", href)
                if match:
                    page_num = int(match.group(1))
                    if page_num > max_page:
                        max_page = page_num
            
            print(f"‚úÖ Total de p√°ginas detectadas: {max_page}")
            return max_page
            
        except Exception as e:
            print(f"‚ùå Error detectando n√∫mero de p√°ginas: {str(e)}")
            return 0
    
    def extract_product_links(self, page_html):
        """Extraer links a productos individuales de una p√°gina"""
        if not page_html:
            print("‚ùå No hay HTML para extraer enlaces de productos")
            return []

        try:
            soup = BeautifulSoup(page_html, 'html.parser')
            
            # An√°lisis de depuraci√≥n b√°sico del HTML
            html_size = len(page_html)
            print(f"üìä Tama√±o del HTML recibido: {html_size} bytes")

            if html_size < 5000:
                print("‚ö†Ô∏è El HTML es muy peque√±o, posiblemente es una p√°gina de error o redirecci√≥n")
                self.debug_page_content(page_html)
                
                # Guardar HTML para an√°lisis
                with open(f"debug_page_{int(time.time())}.html", 'w', encoding='utf-8') as f:
                    f.write(page_html)
                print("üíæ Se ha guardado el HTML para an√°lisis")
                return []
                
            # Buscar enlaces a productos de diferentes maneras
            product_links = []
            
            # M√©todo 1: Enlaces directos a productos_det.asp (principal)
            direct_links = soup.find_all("a", href=lambda href: href and "productos_det.asp" in href)
            print(f"M√©todo 1: Encontrados {len(direct_links)} enlaces directos a productos_det.asp")
            
            for a_tag in direct_links:
                href = a_tag.get("href", "")
                if href.startswith("/"):
                    href = f"{BASE_URL}{href}"
                elif not href.startswith("http"):
                    href = f"{BASE_URL}/{href}"
                product_links.append(href)
            
            # M√©todo 2: Enlaces dentro de divs de producto
            product_containers = soup.select(".product, .item, .product-container, .product-item")
            print(f"M√©todo 2: Encontrados {len(product_containers)} contenedores de producto")
            
            for container in product_containers:
                links = container.find_all("a", href=True)
                for link in links:
                    href = link.get("href", "")
                    if "productos_det.asp" in href:
                        if href.startswith("/"):
                            href = f"{BASE_URL}{href}"
                        elif not href.startswith("http"):
                            href = f"{BASE_URL}/{href}"
                        product_links.append(href)
            
            # M√©todo 3: Enlaces con im√°genes de producto
            img_links = []
            for img in soup.find_all("img"):
                parent_a = img.find_parent("a", href=lambda href: href and "productos_det.asp" in href)
                if parent_a:
                    href = parent_a.get("href", "")
                    if href.startswith("/"):
                        href = f"{BASE_URL}{href}"
                    elif not href.startswith("http"):
                        href = f"{BASE_URL}/{href}"
                    img_links.append(href)
            
            print(f"M√©todo 3: Encontrados {len(img_links)} enlaces con im√°genes de producto")
            product_links.extend(img_links)
            
            # M√©todo 4: Enlaces con texto o contenido relacionado a productos
            product_keywords = ["detalle", "producto", "comprar", "ver", "m√°s info", "m√°s informaci√≥n"]
            potential_product_links = []
            
            for a_tag in soup.find_all("a", href=True):
                text_content = a_tag.get_text().lower()
                
                # Buscar im√°genes dentro del enlace
                has_img = a_tag.find("img") is not None
                has_keyword = any(keyword in text_content for keyword in product_keywords)
                
                if has_img or has_keyword:
                    href = a_tag.get("href", "")
                    if href.startswith("/"):
                        href = f"{BASE_URL}{href}"
                    elif not href.startswith("http"):
                        href = f"{BASE_URL}/{href}"
                    potential_product_links.append(href)
            
            print(f"M√©todo 4: Encontrados {len(potential_product_links)} enlaces potenciales por contenido")
            product_links.extend(potential_product_links)
            
            # M√©todo 5: √öltimo recurso - cualquier enlace con par√°metros de ID
            if len(product_links) < 5:
                id_links = []
                for a_tag in soup.find_all("a", href=lambda href: href and re.search(r'\?.*id=\d+', href or "")):
                    href = a_tag.get("href", "")
                    # Ignorar enlaces obvios de paginaci√≥n o categor√≠as
                    if "pagina=" in href and "productos_det.asp" not in href:
                        continue
                        
                    if href.startswith("/"):
                        href = f"{BASE_URL}{href}"
                    elif not href.startswith("http"):
                        href = f"{BASE_URL}/{href}"
                    id_links.append(href)
                
                print(f"M√©todo 5: Encontrados {len(id_links)} enlaces con par√°metros ID")
                product_links.extend(id_links)
            
            # Eliminar duplicados
            unique_links = list(set(product_links))
            
            # Imprimir ejemplos para depuraci√≥n
            if unique_links:
                print(f"üìå Ejemplos de enlaces encontrados ({len(unique_links)} total):")
                for i, link in enumerate(unique_links[:5]):
                    print(f"  {i+1}. {link}")
                if len(unique_links) > 5:
                    print(f"  ... y {len(unique_links) - 5} m√°s")
            else:
                print("‚ö†Ô∏è No se encontraron enlaces de productos usando ning√∫n m√©todo")
                self.debug_page_content(page_html)
                
                # Guardar HTML para an√°lisis m√°s detallado
                with open(f"debug_page_{int(time.time())}.html", 'w', encoding='utf-8') as f:
                    f.write(page_html)
                print("üíæ Se ha guardado el HTML para an√°lisis detallado")
            
            return unique_links
            
        except Exception as e:
            print(f"‚ùå Error extrayendo enlaces de productos: {str(e)}")
            import traceback
            traceback.print_exc()
            return []
    
    def process_product_page(self, url):
        """Procesar la p√°gina de un producto individual"""
        try:
            print(f"Procesando producto: {url}")
            html = self.fetch_page(url)
            if not html:
                self.stats["errors"] += 1
                return None
                
            soup = BeautifulSoup(html, 'html.parser')
            
            # Extraer ID del producto
            product_id = None
            if "id=" in url:
                product_id = url.split("id=")[1].split("&")[0]
            
            if not product_id:
                print(f"‚ö†Ô∏è No se pudo determinar el ID del producto para {url}")
                self.stats["errors"] += 1
                return None
            
            # Verificar si ya existe
            if product_id in self.existing_products:
                self.stats["existing_products"] += 1
                print(f"‚úÖ Producto {product_id} ya existe en la base de datos")
                return None
            
            # MEJORA 1: Extracci√≥n m√°s precisa del nombre del producto
            name = None

            # M√©todo usado en el scraper original
            try:
                # Intentar usar la misma l√≥gica que en el scraper original
                detail_url = f"{BASE_URL}/productos_full.asp?id={product_id}"
                detail_html = self.fetch_page(detail_url)
                if detail_html:
                    detail_soup = BeautifulSoup(detail_html, 'html.parser')
                    
                    # Buscar tablas con contenido de producto
                    product_tables = detail_soup.find_all("table", id="customers")
                    for table in product_tables:
                        name_cell = table.select_one('tr td[colspan="1"]')
                        if name_cell and name_cell.get_text(strip=True):
                            name = name_cell.get_text(strip=True)
                            print(f"Nombre encontrado usando m√©todo del scraper original: {name}")
                            break
            except Exception as e:
                print(f"Error intentando extraer nombre con m√©todo original: {str(e)}")
                
            # Si el m√©todo original fall√≥, usar m√©todos alternativos
            if not name:
                # M√©todo 1: Buscar en tablas principales (com√∫n en OfertasB)
                tables = soup.find_all("table")
                for table in tables:
                    # Buscar celdas con colspan que suelen contener t√≠tulos de productos
                    name_cells = table.select('tr td[colspan="1"], tr td[colspan="2"], tr td[colspan="3"]')
                    for cell in name_cells:
                        text = cell.get_text(strip=True)
                        if len(text) > 10 and "ofertasb" not in text.lower():
                            name = text
                            print(f"Nombre encontrado en celda de tabla: {name}")
                            break
                    if name:
                        break
            
            # M√©todo 2: Buscar en h1 o h2 principal
            if not name:
                heading_tags = soup.find_all(['h1', 'h2'], limit=3)
                for tag in heading_tags:
                    # Filtrar textos cortos o gen√©ricos
                    text = tag.text.strip()
                    if len(text) > 10 and text.lower() != "ofertasb" and "oferta" not in text.lower():
                        name = text
                        print(f"Nombre encontrado en elemento {tag.name}: {name}")
                        break
                        
            # M√©todo 3: Buscar en elementos espec√≠ficos si no se encontr√≥
            if not name:
                # Intentar varios selectores para el nombre
                for selector in [
                    "div.product-name", 
                    "div.title", 
                    "td.product-name", 
                    ".product-info h1",
                    ".product-details h2",
                    "div.producto-nombre",
                    "div.product-title",
                    "span.product-title"
                ]:
                    name_elem = soup.select_one(selector)
                    if name_elem and len(name_elem.text.strip()) > 10:
                        name = name_elem.text.strip()
                        print(f"Nombre encontrado con selector '{selector}': {name}")
                        break
                    
            # M√©todo 4: Buscar por texto en negrita que podr√≠a ser un t√≠tulo
            if not name:
                bold_elems = soup.find_all(['b', 'strong'])
                for elem in bold_elems:
                    if len(elem.text.strip()) > 10:  # Suficientemente largo para ser un t√≠tulo
                        name = elem.text.strip()
                        print(f"Nombre encontrado en elemento bold: {name}")
                        break
                        
            # M√©todo 5: Buscar texto cercano a "T√≠tulo" o "Producto"
            if not name:
                title_labels = soup.find_all(string=lambda s: s and any(x in s.lower() for x in ["t√≠tulo", "titulo", "producto", "nombre"]))
                for label in title_labels:
                    # Buscar el texto adyacente o siguiente elemento
                    next_elem = label.next_element
                    if next_elem and isinstance(next_elem, str):
                        if len(next_elem.strip()) > 10:
                            name = next_elem.strip()
                            print(f"Nombre encontrado tras etiqueta: {name}")
                            break
                    # O buscar en el padre para casos como <td>T√≠tulo: Producto XYZ</td>
                    parent_text = label.parent.text.strip()
                    if ":" in parent_text:
                        possible_name = parent_text.split(":", 1)[1].strip()
                        if len(possible_name) > 10:
                            name = possible_name
                            print(f"Nombre encontrado tras etiqueta en texto padre: {name}")
                            break
                        
            # Fallback si todav√≠a no se ha encontrado
            if not name:
                name = f"Producto sin nombre {product_id}"
                print(f"‚ö†Ô∏è No se pudo encontrar el nombre del producto {product_id}")
            
            # Asegurarnos de que el nombre no es "Lista de productos" ni otra texto gen√©rico
            if "lista de productos" in name.lower() or "listado de productos" in name.lower():
                name = f"Producto sin categor√≠a {product_id}"
                print(f"‚ö†Ô∏è Nombre gen√©rico detectado, cambiado a: {name}")
            
            # MEJORA 2: Mejor extracci√≥n de precio con s√≠mbolo ‚Ç°
            price_raw = None
            price_numeric = None
            currency = "CRC"  # Por defecto, colones costarricenses
            
            # M√©todo 1: Buscar patrones de precio con s√≠mbolo ‚Ç° directamente en el HTML
            price_patterns = [
                r'‚Ç°\s?[\d.,]+',  # ‚Ç° seguido de n√∫meros
                r'¬¢\s?[\d.,]+',   # ¬¢ seguido de n√∫meros (s√≠mbolo alternativo)
                r'CRC\s?[\d.,]+',  # CRC seguido de n√∫meros
                r'colones\s?[\d.,]+',  # "colones" seguido de n√∫meros
                r'precio:\s?[\d.,]+',  # "precio:" seguido de n√∫meros
                r'precio\s?[\d.,]+',   # "precio" seguido de n√∫meros
                r'cuesta\s?[\d.,]+',   # "cuesta" seguido de n√∫meros
                r'valor\s?[\d.,]+',    # "valor" seguido de n√∫meros
            ]
            
            for pattern in price_patterns:
                match = re.search(pattern, html, re.IGNORECASE)
                if match:
                    price_raw = match.group(0).strip()
                    print(f"Precio encontrado (regex directo): {price_raw}")
                    break
                    
            # M√©todo 2: Buscar en elementos espec√≠ficos si no se encontr√≥ con regex
            if not price_raw:
                price_selectors = [
                    "div.price", "span.price", "div.precio", "span.precio",
                    ".product-price", ".price-value", ".valor", ".monto"
                ]
                
                for selector in price_selectors:
                    price_elem = soup.select_one(selector)
                    if price_elem:
                        price_text = price_elem.text.strip()
                        # Verificar que contiene d√≠gitos
                        if re.search(r'\d', price_text):
                            price_raw = price_text
                            print(f"Precio encontrado ({selector}): {price_raw}")
                            break
            
            # M√©todo 3: Buscar texto cerca de "Precio" o "Valor"
            if not price_raw:
                price_labels = soup.find_all(string=lambda s: s and any(x in s.lower() for x in ["precio", "valor", "costo"]))
                for label in price_labels:
                    parent = label.parent
                    # Buscar texto adyacente
                    if parent.next_sibling and isinstance(parent.next_sibling, str):
                        text = parent.next_sibling.strip()
                        if re.search(r'\d', text):
                            price_raw = text
                            print(f"Precio encontrado en texto adyacente: {price_raw}")
                            break
                    # Buscar siguiente elemento
                    next_elem = parent.next_sibling
                    while next_elem and isinstance(next_elem, str) and not next_elem.strip():
                        next_elem = next_elem.next_sibling
                    
                    if next_elem and hasattr(next_elem, 'text'):
                        text = next_elem.text.strip()
                        if re.search(r'\d', text):
                            price_raw = text
                            print(f"Precio encontrado en elemento adyacente: {price_raw}")
                            break
            
            # M√©todo 4: Buscar en tablas de informaci√≥n
            if not price_raw:
                price_label = soup.find(["td", "th"], string=lambda s: s and any(x in s.lower() for x in ["precio", "valor", "costo"]))
                if price_label:
                    next_cell = price_label.find_next_sibling("td")
                    if next_cell:
                        price_text = next_cell.text.strip()
                        if re.search(r'\d', price_text):
                            price_raw = price_text
                            print(f"Precio encontrado en tabla: {price_raw}")
            
            # Si encontramos un precio, extraer valor num√©rico
            if price_raw:
                try:
                    # A√±adir el s√≠mbolo ‚Ç° si no lo tiene pero contiene n√∫meros
                    if not any(symbol in price_raw for symbol in ['‚Ç°', '¬¢', '$']) and re.search(r'\d', price_raw):
                        price_raw = f"‚Ç°{price_raw}"
                        print(f"Agregado s√≠mbolo ‚Ç° al precio: {price_raw}")
                    
                    # Intentar extraer el valor num√©rico usando nuestra funci√≥n de utilidad
                    price_raw, price_numeric = parse_price(price_raw)
                    print(f"Precio procesado: {price_raw} -> {price_numeric}")
                    
                except ValueError:
                    # Si falla, intentar extracci√≥n directa
                    price_match = re.search(r'[\d\.,]+', price_raw)
                    if price_match:
                        price_str = price_match.group(0).replace(".", "").replace(",", ".")
                        try:
                            price_numeric = float(price_str)
                            print(f"Precio num√©rico extra√≠do manualmente: {price_numeric}")
                        except ValueError:
                            price_numeric = None
                            print(f"‚ö†Ô∏è No se pudo convertir {price_str} a n√∫mero")
            
            # Si no se encuentra precio num√©rico o contiene "negociable", establecer correctamente
            if not price_numeric:
                # Verificar si tiene texto sobre negociaci√≥n
                if any(term in html.lower() for term in ["negociable", "a convenir", "consultar precio"]):
                    price_raw = "Negociable con vendedor"
                    price_numeric = None
                    print(f"Precio detectado como negociable")
                else:
                    price_raw = "Precio no disponible"
                    price_numeric = None
                    print(f"‚ö†Ô∏è No se pudo encontrar un precio")
            
            # MEJORA 3: Mejor extracci√≥n y almacenamiento de im√°genes en carpeta espec√≠fica
            image_url = None
            image_stored_url = None
            
            # M√©todo 1: Buscar en div#content
            main_content = soup.find('div', {'id': 'content'})
            if main_content:
                img = main_content.find('img', src=lambda x: x and ('upload' in x or 'images' in x))
                if img and img.get('src'):
                    image_url = img['src']
                    print(f"Imagen encontrada (content): {image_url}")
            
            # M√©todo 2: Buscar en div.product-image
            if not image_url:
                product_image_div = soup.find('div', class_='product-image')
                if product_image_div:
                    img = product_image_div.find('img', src=True)
                    if img:
                        image_url = img['src']
                        print(f"Imagen encontrada (product-image): {image_url}")
            
            # M√©todo 3: Buscar im√°genes grandes que puedan ser de producto
            if not image_url:
                all_images = soup.find_all('img', src=True)
                product_images = [img for img in all_images if any(term in img.get('src', '') for term in 
                                 ['product', 'prod', 'item', 'foto', 'image', 'img', 'upload'])]
                
                if product_images:
                    # Ordenar por tama√±o del src (generalmente las im√°genes de producto tienen URLs m√°s largas)
                    product_images.sort(key=lambda img: len(img.get('src', '')), reverse=True)
                    image_url = product_images[0]['src']
                    print(f"Imagen encontrada (por nombre): {image_url}")
                elif all_images:
                    # Si no encontramos im√°genes espec√≠ficas de producto, usar la m√°s grande
                    largest_img = max(all_images, key=lambda img: len(img.get('src', '')))
                    image_url = largest_img['src']
                    print(f"Imagen encontrada (la m√°s grande): {image_url}")
            
            # Normalizar URL de imagen
            if image_url:
                if image_url.startswith('/'):
                    image_url = f"{BASE_URL}{image_url}"
                elif not image_url.startswith('http'):
                    image_url = f"{BASE_URL}/{image_url}"
                print(f"URL de imagen normalizada: {image_url}")
                
                # Descargar y guardar la imagen en Supabase Storage en carpeta espec√≠fica
                try:
                    # Descargar la imagen
                    response = self.session.get(image_url)
                    if response.status_code == 200:
                        # Generar nombre de archivo √∫nico
                        image_extension = image_url.split('.')[-1] if '.' in image_url else 'jpg'
                        if len(image_extension) > 4 or not image_extension.isalpha():  # Si la extensi√≥n es inv√°lida
                            image_extension = 'jpg'
                            
                        filename = f"{product_id}_{uuid.uuid4().hex[:8]}.{image_extension}"
                        storage_path = f"{UNCATEGORIZED_FOLDER}/{filename}"
                        
                        # Guardar en Supabase Storage
                        image_data = response.content
                        
                        try:
                            # Intentar subir la imagen
                            result = self.supabase.storage.from_(IMAGE_BUCKET).upload(
                                path=storage_path,
                                file=image_data,
                                file_options={"content-type": f"image/{image_extension}"}
                            )
                            
                            # Si llegamos aqu√≠, la carga fue exitosa
                            # Obtener la URL p√∫blica
                            try:
                                public_url = self.supabase.storage.from_(IMAGE_BUCKET).get_public_url(storage_path)
                                print(f"‚úÖ Imagen guardada en Supabase: {storage_path}")
                                print(f"URL p√∫blica: {public_url}")
                                image_stored_url = public_url
                                self.stats["images_saved"] += 1
                            except Exception as url_error:
                                print(f"‚ö†Ô∏è Error obteniendo URL p√∫blica: {str(url_error)}")
                                self.stats["image_errors"] += 1
                                
                        except Exception as upload_error:
                            print(f"‚ö†Ô∏è Error durante la carga: {str(upload_error)}")
                            self.stats["image_errors"] += 1
                    else:
                        print(f"‚ö†Ô∏è Error descargando imagen, status: {response.status_code}")
                        self.stats["image_errors"] += 1
                        
                except Exception as e:
                    print(f"‚ùå Error procesando imagen: {str(e)}")
                    self.stats["image_errors"] += 1
            
            # VERIFICACI√ìN DE CATEGOR√çA - PARTE CR√çTICA
            is_uncategorized = False
            category_id = None
            category_name = None
            
            # M√©todo 1: Buscar por texto "Categor√≠a" en tablas
            category_info = soup.find(["td", "th"], string=lambda s: s and "categor√≠a" in s.lower())
            if category_info:
                print("Encontrada referencia a 'Categor√≠a' en una tabla")
                # Encontrar el valor correspondiente
                category_value = category_info.find_next_sibling(["td", "th"])
                if category_value:
                    category_text = category_value.text.strip().lower()
                    print(f"Texto de categor√≠a encontrado: '{category_text}'")
                    
                    # Verificar si es "sin categor√≠a"
                    if "sin categor√≠a" in category_text or "sin categoria" in category_text:
                        is_uncategorized = True
                        print("‚úÖ PRODUCTO SIN CATEGOR√çA ENCONTRADO!")
                    
                    # Intentar extraer el ID de categor√≠a de la URL si existe
                    category_link = category_value.find("a", href=lambda href: href and "productos_cat.asp" in href)
                    if category_link:
                        href = category_link.get("href", "")
                        match = re.search(r"id=(\d+)", href)
                        if match:
                            category_id = match.group(1)
                            print(f"ID de categor√≠a extra√≠do de la URL: {category_id}")
                            
                    # Capturar el nombre de la categor√≠a
                    category_name = category_value.get_text(strip=True)
            
            # M√©todo 2: Buscar directamente texto "sin categor√≠a" en la p√°gina
            if not is_uncategorized:
                # Buscar en el texto completo de la p√°gina
                page_text = soup.get_text().lower()
                if "sin categor√≠a" in page_text or "sin categoria" in page_text:
                    is_uncategorized = True
                    print("‚úÖ Texto 'sin categor√≠a' encontrado en la p√°gina!")
                
                # Buscar en elementos espec√≠ficos que podr√≠an contener la categor√≠a
                category_elements = soup.select(".category, .product-category, .breadcrumb")
                for elem in category_elements:
                    elem_text = elem.get_text().lower()
                    if "sin categor√≠a" in elem_text or "sin categoria" in elem_text:
                        is_uncategorized = True
                        print(f"‚úÖ 'Sin categor√≠a' encontrado en elemento {elem.name}.{elem.get('class', '')}")
                        break
            
            # M√©todo 3: Buscar breadcrumb de navegaci√≥n
            if not category_id and not is_uncategorized:
                breadcrumbs = soup.find_all(["div", "nav", "ul"], class_=lambda c: c and "breadcrumb" in c.lower())
                for breadcrumb in breadcrumbs:
                    links = breadcrumb.find_all("a")
                    for link in links:
                        href = link.get("href", "")
                        if "categoria" in href.lower() or "productos_cat.asp" in href.lower():
                            match = re.search(r"id=(\d+)", href)
                            if match:
                                category_id = match.group(1)
                                category_name = link.text.strip()
                                print(f"Categor√≠a encontrada en breadcrumb: {category_name} (ID: {category_id})")
                                break
            
            # Si despu√©s de todo no se encontr√≥ categor√≠a expl√≠cita, verificar si tiene marcadores
            # comunes de productos sin categorizar
            if not is_uncategorized and not category_id:
                uncategorized_markers = [
                    "producto sin clasificar",
                    "sin clasificaci√≥n",
                    "no categorizado",
                    "categor√≠a: n/a",
                    "categor√≠a: ninguna"
                ]
                
                page_text = soup.get_text().lower()
                for marker in uncategorized_markers:
                    if marker in page_text:
                        is_uncategorized = True
                        print(f"‚úÖ Marcador de producto sin categor√≠a encontrado: '{marker}'")
                        break
            
            # Crear datos del producto
            product_data = {
                "external_product_id": product_id,
                "name": name,
                "product_url": url,
                "image_url": image_stored_url or image_url,  # Usar URL de Supabase si est√° disponible
                "price_raw": price_raw,
                "price_numeric": price_numeric,
                "currency": currency,
                "first_seen_at": datetime.utcnow().isoformat(),
                "last_seen_at": datetime.utcnow().isoformat(),
                "seller_id": 1,
                "source_html_hash": hashlib.md5(html.encode('utf-8')).hexdigest()
            }
            
            # Guardar HTML completo para referencia, pero no en la base de datos
            product_data["source_html"] = html
            
            # Asignar categor√≠a basado en nuestro an√°lisis
            if is_uncategorized:
                product_data["category_id"] = self.uncategorized_category["id"]
                self.stats["uncategorized_products"] += 1
                print(f"üéØ Producto {product_id} confirmado como 'Sin categor√≠a'")
                
                # Guardar una copia del HTML para an√°lisis (solo si es sin categor√≠a, para optimizar espacio)
                with open(f"uncategorized_product_{product_id}.html", 'w', encoding='utf-8') as f:
                    f.write(html)
                print(f"üíæ Guardado HTML de producto sin categor√≠a: {product_id}")
                
            elif category_id and category_id in self.category_map:
                # Este producto tiene una categor√≠a v√°lida, lo ignoramos para este scraper espec√≠fico
                print(f"‚è≠Ô∏è Producto {product_id} tiene categor√≠a asignada: {self.category_map[category_id]['name']}")
                return None
            else:
                # No pudimos determinar si es sin categor√≠a o no, por lo tanto lo consideramos sin categor√≠a
                product_data["category_id"] = self.uncategorized_category["id"]
                self.stats["uncategorized_products"] += 1
                print(f"üîç Producto {product_id} sin categor√≠a clara, asignado a 'Sin categor√≠a'")
            
            # Actualizar estad√≠sticas solo si vamos a guardarlo (es decir, si es sin categor√≠a)
            self.stats["total_products"] += 1
            self.stats["new_products"] += 1
            
            return product_data
            
        except Exception as e:
            print(f"‚ùå Error procesando producto {url}: {str(e)}")
            import traceback
            traceback.print_exc()
            self.stats["errors"] += 1
            return None
    
    def save_product(self, product_data):
        """Guardar un producto en Supabase"""
        if not product_data:
            return False
            
        try:
            # Quitar el HTML completo para el upsert (lo guardamos separado)
            html_content = product_data.pop("source_html", None)
            
            # Insertar el producto
            result = self.supabase.table("products").insert(product_data).execute()
            
            if not result.data:
                print(f"‚ùå Error guardando producto {product_data.get('external_product_id')}")
                return False
                
            # Marcar como existente en nuestro cache
            self.existing_products[product_data["external_product_id"]] = True
            
            return True
            
        except Exception as e:
            print(f"‚ùå Error guardando producto {product_data.get('external_product_id')}: {str(e)}")
            return False
    
    def debug_page_content(self, html):
        """Mostrar informaci√≥n de depuraci√≥n sobre el contenido de la p√°gina"""
        if not html:
            print("‚ö†Ô∏è HTML vac√≠o o nulo")
            return
            
        soup = BeautifulSoup(html, 'html.parser')
        
        # Extraer y mostrar t√≠tulo
        title = soup.title.text if soup.title else "Sin t√≠tulo"
        print(f"üìÑ T√≠tulo de la p√°gina: {title}")
        
        # Mostrar elementos principales
        print("üìã Elementos principales:")
        for tag_name in ['h1', 'h2', 'form']:
            elements = soup.find_all(tag_name)
            if elements:
                print(f"  - {tag_name}: {len(elements)} elementos")
                for i, elem in enumerate(elements[:3]):
                    print(f"    {i+1}. {elem.get_text().strip()[:50]}")
                if len(elements) > 3:
                    print(f"    ... y {len(elements) - 3} m√°s")
        
        # Mostrar posibles mensajes de error o redirecci√≥n
        error_indicators = [
            "error", "no encontrado", "not found", "404", "403", 
            "mantenimiento", "maintenance", "redirect", "redirection"
        ]
        
        for indicator in error_indicators:
            if indicator in html.lower():
                print(f"‚ö†Ô∏è Posible problema detectado: '{indicator}'")
    
    def run(self, page_limit=None, start_page=1, debug_mode=False):
        """Ejecutar el scraper completo
        
        Args:
            page_limit (int, optional): Limitar la ejecuci√≥n a este n√∫mero de p√°ginas
            start_page (int, optional): P√°gina desde la que comenzar
            debug_mode (bool, optional): Si es True, muestra informaci√≥n adicional de depuraci√≥n
        """
        start_time = time.time()
        try:
            print("=" * 80)
            print("üîç INICIANDO SCRAPER DE PRODUCTOS SIN CATEGOR√çA")
            print(f"üìÖ Fecha y hora: {datetime.now().strftime('%Y-%m-%d %H:%M:%S')}")
            print("=" * 80)
            
            # Crear categor√≠a "Sin categor√≠a"
            self.create_uncategorized_category()
            
            # Cargar mapeo de categor√≠as
            self.load_categories_map()
            
            # Cargar productos existentes
            self.load_existing_products()
            
            # Obtener el n√∫mero total de p√°ginas
            total_pages = self.get_total_pages()
            if total_pages == 0:
                print("‚ùå No se pudieron detectar las p√°ginas. Abortando.")
                return
                
            # Si se especific√≥ un l√≠mite, ajustar el n√∫mero de p√°ginas
            if page_limit and page_limit > 0:
                end_page = min(start_page + page_limit - 1, total_pages)
                print(f"‚ö†Ô∏è Limitando de p√°gina {start_page} a {end_page} (de un total de {total_pages})")
                total_pages_to_process = end_page - start_page + 1
            else:
                end_page = total_pages
                total_pages_to_process = total_pages - start_page + 1
            
            print(f"\nüöÄ Procesando {total_pages_to_process} p√°ginas (de {start_page} a {end_page})")
            
            # Configurar barra de progreso para las p√°ginas
            page_progress = tqdm.tqdm(total=total_pages_to_process, desc="Procesando p√°ginas", unit="p√°gina")
            
            # Variables de control para detecci√≥n de problemas
            consecutive_empty_pages = 0
            max_consecutive_empty = 5  # Detener despu√©s de 5 p√°ginas consecutivas sin productos
            
            # Procesar cada p√°gina
            for page_num in range(start_page, end_page + 1):
                page_url = f"{BASE_URL}/productos_cat.asp?pagina={page_num}"
                print(f"\n{'='*40}")
                print(f"üåê P√ÅGINA {page_num}/{end_page}: {page_url}")
                print(f"{'='*40}")
                
                page_html = self.fetch_page(page_url)
                if not page_html:
                    page_progress.update(1)
                    consecutive_empty_pages += 1
                    print(f"‚ö†Ô∏è P√°gina {page_num} sin contenido ({consecutive_empty_pages} consecutivas)")
                    
                    if consecutive_empty_pages >= max_consecutive_empty:
                        print(f"‚õî {max_consecutive_empty} p√°ginas consecutivas sin contenido. Finalizando.")
                        break
                        
                    continue
                
                # An√°lisis previo del HTML
                if debug_mode:
                    print("\nüî¨ An√°lisis previo del HTML de la p√°gina:")
                    self.debug_page_content(page_html)
                
                # Extraer links a productos
                product_links = self.extract_product_links(page_html)
                
                if product_links:
                    consecutive_empty_pages = 0  # Reiniciar contador si encontramos productos
                    print(f"‚úÖ Encontrados {len(product_links)} productos en p√°gina {page_num}")
                else:
                    consecutive_empty_pages += 1
                    print(f"‚ö†Ô∏è No se encontraron productos en p√°gina {page_num} ({consecutive_empty_pages} consecutivas)")
                    
                    # Si tenemos demasiadas p√°ginas consecutivas sin productos, algo puede estar mal
                    if consecutive_empty_pages >= max_consecutive_empty:
                        print(f"‚õî {max_consecutive_empty} p√°ginas consecutivas sin productos. Finalizando.")
                        
                        # Guardar √∫ltima p√°gina para an√°lisis
                        with open(f"debug_empty_page_{page_num}.html", 'w', encoding='utf-8') as f:
                            f.write(page_html)
                        print(f"üíæ Guardada p√°gina vac√≠a para an√°lisis: debug_empty_page_{page_num}.html")
                        
                        break
                
                # Procesar cada producto
                if product_links:
                    product_progress = tqdm.tqdm(total=len(product_links), desc=f"Productos en p√°gina {page_num}", unit="producto")
                    uncategorized_in_page = 0
                    
                    for product_url in product_links:
                        # Procesar producto
                        product_data = self.process_product_page(product_url)
                        
                        # Guardar si es v√°lido (sin categor√≠a)
                        if product_data:
                            self.save_product(product_data)
                            uncategorized_in_page += 1
                        
                        product_progress.update(1)
                    
                    product_progress.close()
                    print(f"üìä Resumen de la p√°gina {page_num}: {uncategorized_in_page} productos sin categor√≠a de {len(product_links)} totales")
                
                page_progress.update(1)
                
                # Peque√±a pausa entre p√°ginas para evitar sobrecargar el servidor
                time.sleep(random.uniform(1.5, 3.0))
            
            page_progress.close()
            
            # Duraci√≥n total
            duration = time.time() - start_time
            hours, remainder = divmod(duration, 3600)
            minutes, seconds = divmod(remainder, 60)
            
            # Mostrar estad√≠sticas finales
            print("\n" + "="*50)
            print("üìä ESTAD√çSTICAS FINALES")
            print("="*50)
            print(f"‚úÖ Total de productos procesados: {self.stats['total_products']}")
            print(f"üìã Productos sin categor√≠a: {self.stats['uncategorized_products']}")
            print(f"üÜï Productos nuevos: {self.stats['new_products']}")
            print(f"üîÑ Productos existentes (ignorados): {self.stats['existing_products']}")
            print(f"üñºÔ∏è Im√°genes guardadas: {self.stats['images_saved']}")
            print(f"‚ùå Errores de im√°genes: {self.stats['image_errors']}")
            print(f"‚ö†Ô∏è Errores generales: {self.stats['errors']}")
            print(f"‚è±Ô∏è Duraci√≥n total: {int(hours)}h {int(minutes)}m {int(seconds)}s")
            print("="*50)
            
        except KeyboardInterrupt:
            print("\n\n‚õî Ejecuci√≥n interrumpida por el usuario")
            
            # Duraci√≥n hasta interrupci√≥n
            duration = time.time() - start_time
            hours, remainder = divmod(duration, 3600)
            minutes, seconds = divmod(remainder, 60)
            
            # Mostrar estad√≠sticas parciales
            print("\n" + "="*50)
            print("üìä ESTAD√çSTICAS PARCIALES")
            print("="*50)
            print(f"‚úÖ Productos procesados: {self.stats['total_products']}")
            print(f"üìã Productos sin categor√≠a: {self.stats['uncategorized_products']}")
            print(f"üÜï Productos nuevos: {self.stats['new_products']}")
            print(f"üîÑ Productos existentes: {self.stats['existing_products']}")
            print(f"üñºÔ∏è Im√°genes guardadas: {self.stats['images_saved']}")
            print(f"‚ùå Errores de im√°genes: {self.stats['image_errors']}")
            print(f"‚ö†Ô∏è Errores: {self.stats['errors']}")
            print(f"‚è±Ô∏è Duraci√≥n: {int(hours)}h {int(minutes)}m {int(seconds)}s")
            print("="*50)
            
        except Exception as e:
            print(f"\n‚ùå Error durante la ejecuci√≥n: {str(e)}")
            import traceback
            traceback.print_exc()


if __name__ == "__main__":
    # Configurar argumentos de l√≠nea de comandos
    parser = argparse.ArgumentParser(description='Scraper de productos sin categor√≠a de OfertasB')
    parser.add_argument('-l', '--limit', type=int, help='Limitar a un n√∫mero espec√≠fico de p√°ginas')
    parser.add_argument('-s', '--start', type=int, default=1, help='P√°gina desde la que comenzar')
    parser.add_argument('-d', '--debug', action='store_true', help='Modo debug con informaci√≥n adicional')
    args = parser.parse_args()
    
    # Crear e iniciar el scraper
    scraper = UncategorizedScraper()
    scraper.run(page_limit=args.limit, start_page=args.start, debug_mode=args.debug)
